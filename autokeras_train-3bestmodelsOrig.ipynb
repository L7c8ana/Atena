{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ac03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classificou bem classe 2 com 100 %\n",
    "\n",
    "\n",
    "!pip install autokeras\n",
    "!pip install PyQt5\n",
    "!pip install scikit-image\n",
    "!apt-get update\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y\n",
    "!pip install seaborn\n",
    "import random, os, copy, sys\n",
    "import os\n",
    "import pickle\n",
    "import autokeras as ak\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Add, Lambda #, MaxPooling2D, Conv2D, Flatten, Concatenate, Add, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Layer\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# snippet of using the ReduceLROnPlateau callback\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe8be3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#get_ipython().system('ls -l /tf/DataProjects/NewBoreholeMarcio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a805f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# G = [\"8\", \"9\"]\n",
    "G = [\"1\"]\n",
    "stringGPUs = []\n",
    "for i in G:\n",
    "    GPS = \"GPU:\" + i\n",
    "    stringGPUs.append(GPS)\n",
    "\n",
    "\n",
    "print(stringGPUs)\n",
    "print(\",\".join(G))\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(G)\n",
    "\n",
    "if len(G) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(devices=stringGPUs, cross_device_ops=tf.distribute.NcclAllReduce())\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=stringGPUs[0])\n",
    "\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6b453",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!mv /tf/DataProjects/NewBoreholeMarcio/dataNormW_0.pkl /tf/DataProjects/NewBoreholeMarcio/dataNormW1_W3_W9.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69692291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_dirSave = \"/tf/DataProjects/NewBoreholeMarcio\"\n",
    "#import sys\n",
    "#sys.path.insert(0, os.path.join(data_dirSave,\"EntregaPetroDissolucao\"))\n",
    "from data_loader import load_dataClassNewData #load_dataClassNewMarcioTrain#, load_dataClass2Borehole\n",
    "# data_dirSave = \"/tf/DataProjects/NewBoreholeMarcio\"\n",
    "DataIMGChannel = [0,1,2]\n",
    "AKCustom = False\n",
    "ClssDA = 2\n",
    "dataPred = False\n",
    "Train = False\n",
    "max_trials = 5\n",
    "epochs = 100\n",
    "batch_size = 16 * len(G)\n",
    "window_size = 41\n",
    "DataModel = None # [os.path.join(data_dirSave, \"W1_5315-5738m_AMP.npy\")]#, os.path.join(data_dirSave, \"W3_5314-5883m_AMP_RAW.npy\"), os.path.join(data_dirSave, \"W9_AMP.npy\")]\n",
    "# sep = [\";\", \";\", \",\"]#, \";\", \",\", \",\", \",\"]#[\",\", \",\", \";\", \";\", \";\", \";\", \",\", \",\", \",\", \",\", \",\"]\n",
    "IDx1Borehole = [\"W9\"] #[\"W1\", \"W3\", \"W9\"]#, \"W6\", \"W8\", \"W10\", \"W11\", \"W12\"]#[\"W1\", \"W2\", \"W3\", \"W5\", \"W6\", \"W8\", \"W9\", \"W10\", \"W11\", \"W12\", \"W13\"]\n",
    "IDx2Borehole = [\"_AMP\"] #[\"_5315-5738m_AMP\", \"_5314-5883m_AMP_RAW\", \"_AMP\"]#, \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\"]#[\"_5315-5738m_AMP\", \"_5485_5915m_AMP\", \"_5314-5883m_AMP_RAW\", \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\", \"_AMP\"]\n",
    "# ft = []\n",
    "ReloadPKL = \"None\"\n",
    "nb_classes = 3\n",
    "data_dirIMG = []\n",
    "data_dirClass = []\n",
    "titlefoldsave = \"TEST3BestModelsCLASSESinW1\"\n",
    "if AKCustom:\n",
    "    PathSaveFinals = os.path.join(data_dirSave, \"ResultCONCTRAINWS\" + str(window_size) + titlefoldsave)\n",
    "else:\n",
    "    PathSaveFinals = os.path.join(data_dirSave, \"ResultCONCTRAINWS\" + str(window_size) + titlefoldsave)\n",
    "    \n",
    "\n",
    "foldermodel0 = os.path.join(data_dirSave, \"ResultTRAINWS41AKDissolutionClipPercentileLog0W1\")\n",
    "foldermodel1 = os.path.join(data_dirSave, \"ResultTRAINWS41AKDissolutionClipPercentileOrig1W1\")\n",
    "foldermodel2 = os.path.join(data_dirSave, \"ResultTRAINWS41AKDissolutionClipPercentileGauss2W1\")    \n",
    "DataModelTrain = os.path.join(data_dirSave, \"ResultCONCTRAINWS\" + str(window_size) + \"Train3BestModelsCLASSESinW1\")\n",
    "if not os.path.exists(PathSaveFinals):\n",
    "    os.mkdir(PathSaveFinals)\n",
    "\n",
    "for s, n, m in zip(range(len(IDx2Borehole)),IDx1Borehole, IDx2Borehole):\n",
    "    h = n + m\n",
    "    data_dirIMG0 = os.path.join(data_dirSave, h + \".csv\")\n",
    "    data_dirClass0 = os.path.join(data_dirSave, n + \"_FACIESRODAAN.csv\")\n",
    "    data_dirIMG.append(data_dirIMG0)\n",
    "    data_dirClass.append(data_dirClass0)\n",
    "   \n",
    "data_norm = load_dataClassNewData(ReloadPKL=ReloadPKL, data_dirIMG=data_dirIMG, data_dirClass=data_dirClass, data_dirSave=data_dirSave,\n",
    "                                           MakeChannels=True, dataPred=dataPred, DataTrain=DataModelTrain, Train=Train, LabelWell=IDx1Borehole)\n",
    "\n",
    "# Nao por essa celula antes do load\n",
    "#import sys\n",
    "#sys.path.insert(0, os.path.abspath(os.path.join(data_dirSave,'EntregaPetroDissolucao','BoreholeTools')))\n",
    "#print(\"Folder Actual: \" + os.getcwd())\n",
    "#from SlidingDataGenerator import SlidingDataGenerator\n",
    "#from BoreholeImporter import Data\n",
    "# from data_loader import load_dataClass, load_data\n",
    "\n",
    "from utils import ismember, cm_analysis, divideblocksizeTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c1670",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def uniqueStr(l):\n",
    "    u = list(set(l))\n",
    "    i = []\n",
    "    for c in l:\n",
    "        i.append(int(np.float32(c)))\n",
    "        \n",
    "    return u,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d287f41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#orig = data_norm['Acoustic'][:,:,0].reshape(data_norm['Acoustic'].shape[0], data_norm['Acoustic'].shape[1], 1) \n",
    "data_norm['Acoustic'][:,:,1] = np.nan_to_num(data_norm['Acoustic'][:,:,1], nan=0.0)\n",
    "#log = data_norm['Acoustic'][:,:,1].reshape(orig.shape[0], orig.shape[1], 1) \n",
    "#print(log.max(), log.min())\n",
    "#print(orig.max(), orig.min())\n",
    "# plt.figure()\n",
    "# plt.imshow(orig[0:100,:])\n",
    "# plt.show()\n",
    "# plt.figure()\n",
    "# plt.imshow(log[0:100,:])\n",
    "# plt.show()\n",
    "#gauss = data_norm['Acoustic'][:,:,2].reshape(orig.shape[0], orig.shape[1], 1) \n",
    "#print(gauss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee301bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "X = data_norm['Acoustic'][:,:,np.array(DataIMGChannel)]#.reshape(orig.shape[0], orig.shape[1], 1)\n",
    "#X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(X.shape)\n",
    "\n",
    "# print(data_norm['FDISSOL'])\n",
    "# print(type(data_norm['FDISSOL']))\n",
    "Ylab = data_norm['FDISSOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7358ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Ylevels, Ynum = uniqueStr(Ylab)\n",
    "\n",
    "#nb_classes = len(Ylevels)\n",
    "\n",
    "__M__ = np.shape(X)[0]\n",
    "\n",
    "\n",
    "\n",
    "LabelBH = \"_\".join(IDx1Borehole)\n",
    "if Train:\n",
    "    train_split, test_split, validation_split = 0.6, 0.2, 0.2\n",
    "else:\n",
    "    train_split, test_split, validation_split = 0.0, 1.0, 0.0\n",
    "\n",
    "xtrain, Ytrain, xtest, Ytest, xval, Yval, ids = divideblocksizeTensor(X, Ynum, window_size, train_split, test_split, validation_split,LabelBH)\n",
    "#xtrain_Log, Ytrain, xtest_Log, Ytest, xval_Log, Yval, ids = divideblocksizeTensor(log, Ynum, window_size, train_split, test_split, validation_split)\n",
    "#xtrain_Gauss, Ytrain, xtest_Gauss, Ytest, xval_Gauss, Yval, ids = divideblocksizeTensor(gauss, Ynum, window_size, train_split, test_split, validation_split)\n",
    "       \n",
    "print(xtrain.shape, xtest.shape, xval.shape)\n",
    "print(Ytrain.shape, Ytest.shape, Yval.shape)\n",
    "\n",
    "\n",
    "ytrain = to_categorical(Ytrain, num_classes=nb_classes)\n",
    "ytest = to_categorical(Ytest, num_classes=nb_classes)\n",
    "yval = to_categorical(Yval, num_classes=nb_classes)\n",
    "print(\"#############\")\n",
    "print(ytrain.shape, ytest.shape, yval.shape)\n",
    "# create data augmentation classe 2 only data train\n",
    "# Funcoes para geracao de imagens\n",
    "ytrain1label = Ytrain\n",
    "\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "def h_flip(image):\n",
    "    '''\n",
    "    Função responsável por fazer a inversão horizontal da imagem.\n",
    "    Entrada: Imagem\n",
    "    Saída: Imagem invertida no sentido horizontal\n",
    "    '''\n",
    "    return np.fliplr(image)\n",
    "\n",
    "def v_flip(image):\n",
    "    '''\n",
    "    Função responsável por fazer a inversão vertical da imagem.\n",
    "    Entrada: Imagem\n",
    "    Saída: Imagem invertida no sentido vertical\n",
    "    '''\n",
    "    return np.flipud(image)\n",
    "\n",
    "# Dicionario para ativacao das funcoes\n",
    "transformations = {'Horizontal flip': h_flip,\n",
    "                   'Vertical flip': v_flip,\n",
    "                  }\n",
    "#############################################################################################\n",
    "IDXCasos = [n for n, cl in enumerate(ytrain1label) if cl == 1 or cl == 2]\n",
    "print(len(IDXCasos))\n",
    "\n",
    "CLS = []\n",
    "ccc = 0\n",
    "DataNormAugOrig = np.zeros((len(IDXCasos)*len(transformations),xtrain.shape[1], xtrain.shape[2], xtrain.shape[3]))\n",
    "for n,i in enumerate(IDXCasos):\n",
    "    I = xtrain[i]\n",
    "    for m,j in enumerate(transformations):\n",
    "        if m == 0:\n",
    "            TfI = h_flip(I)\n",
    "            ttl = \"Horizontal flip\"\n",
    "            clss = ytrain[i]\n",
    "        else:\n",
    "            TfI = v_flip(I)\n",
    "            ttl = \"Vertical flip\"\n",
    "            clss = ytrain[i]\n",
    "        DataNormAugOrig[ccc] = TfI\n",
    "        ccc += 1\n",
    "        \n",
    "        CLS.append(clss)\n",
    "\n",
    "# CLS = np.array(CLS)\n",
    "# print(CLS.shape)\n",
    "ytrainDA = np.array(CLS)#tf.keras.utils.to_categorical(CLS, num_classes=3, dtype='float32')\n",
    "print(ytrainDA.shape)\n",
    "if Train:\n",
    "    xtrain = np.concatenate((xtrain,DataNormAugOrig), axis=0)# classifica bem a classe 2\n",
    "    ytrain = np.concatenate((ytrain,ytrainDA), axis=0)# classifica bem a classe 2\n",
    "#########################################################################################################\n",
    "\n",
    "    print(xtrain.shape)\n",
    "    print(ytrain.shape)\n",
    "\n",
    "#model0 = load_model(os.path.join(foldermodel0, \"model_autokerasWS\" + str(window_size)))# gauss\n",
    "#model1 = load_model(os.path.join(foldermodel1, \"model_autokerasWS\" + str(window_size)))#log \n",
    "#model2 = load_model(os.path.join(foldermodel2, \"model_autokerasWS\" + str(window_size)))#orig\n",
    "\n",
    "if Train:\n",
    "    with strategy.scope():\n",
    "        model0 = load_model(os.path.join(foldermodel0, \"model_autokerasWS\" + str(window_size)))# gauss\n",
    "        model1 = load_model(os.path.join(foldermodel1, \"model_autokerasWS\" + str(window_size)))#log \n",
    "        model2 = load_model(os.path.join(foldermodel2, \"model_autokerasWS\" + str(window_size)))#orig\n",
    "        model0.summary()\n",
    "\n",
    "        model1.summary()\n",
    "        model2.summary()\n",
    "\n",
    "    #initializer0 = tensorflow.keras.initializers.GlorotUniform()\n",
    "\n",
    "    #def reinitialize_layer(model, initializer, layer_name, trainable=False):\n",
    "    #    layer = model.get_layer(layer_name)\n",
    "    #    layer.trainable = trainable\n",
    "    #    layer.set_weights([initializer(shape=w.shape) for w in layer.get_weights()])\n",
    "    #    return model\n",
    "\n",
    "\n",
    "                          \n",
    "    #for n, i in enumerate(model0.layers):\n",
    "    #    if 0:#n == 6:\n",
    "    #        for j in model0.layers[n].layers:\n",
    "    #             print(j.name)\n",
    "    #            if j.name[-3:] == \"_bn\":# or j.name[-12:] == \"ormalization\":\n",
    "    #                if \"kernel_initializer\" in j.get_config():\n",
    "    #                    initializer = j.kernel_initializer\n",
    "    #                    reinitialize_layer(model1.layers[n], initializer, j.name, trainable=True)\n",
    "    #                else:\n",
    "    #                    reinitialize_layer(model1.layers[n], initializer0, j.name, trainable=True)\n",
    "        #else:\n",
    "        #    if \"kernel_initializer\" in model1.layers[n].get_config():\n",
    "        #        initializer = model1.layers[n].kernel_initializer\n",
    "        #        reinitialize_layer(model1, initializer, model1.layers[n].name, trainable=True)\n",
    "        #    else:\n",
    "        #        reinitialize_layer(model1, initializer0, model1.layers[n].name, trainable=True)\n",
    "                \n",
    "    #for n, i in enumerate(model1.layers):\n",
    "    #    if 0:#n == 5:\n",
    "    #        for j in model1.layers[n].layers:\n",
    "    #             print(j.name)\n",
    "    #            if j.name[-3:] == \"_bn\":#or j.name[-12:] == \"ormalization\":\n",
    "    #                 print(j.name[-3:])\n",
    "    #                 print(j.name[-12:])\n",
    "    #                \n",
    "    #                if \"kernel_initializer\" in j.get_config():\n",
    "    #                    initializer = j.kernel_initializer\n",
    "    #                    reinitialize_layer(model2.layers[n], initializer, j.name, trainable=True)\n",
    "    #                else:\n",
    "    #                    reinitialize_layer(model2.layers[n], initializer0, j.name, trainable=True)\n",
    "\n",
    "\n",
    "        #else:\n",
    "        #    if \"kernel_initializer\" in model2.layers[n].get_config():\n",
    "        #        initializer = model2.layers[n].kernel_initializer\n",
    "        #        reinitialize_layer(model2, initializer, model2.layers[n].name, trainable=True)\n",
    "        #    else:\n",
    "        #        reinitialize_layer(model2, initializer0, model2.layers[n].name, trainable=True)\n",
    "                \n",
    "    #for n, i in enumerate(model2.layers):\n",
    "    #    if 0:#n == :\n",
    "    #        for j in model2.layers[n].layers:\n",
    "    #             print(j.name)\n",
    "    #            if j.name[-3:] == \"_bn\":#or j.name[-12:] == \"ormalization\":\n",
    "    #                 print(j.name[-3:])\n",
    "    #                 print(j.name[-12:])\n",
    "                    \n",
    "    #                if \"kernel_initializer\" in j.get_config():\n",
    "    #                    initializer = j.kernel_initializer\n",
    "    #                    reinitialize_layer(model3.layers[n], initializer, j.name, trainable=True)\n",
    "    #                else:\n",
    "    #                    reinitialize_layer(model3.layers[n], initializer0, j.name, trainable=True)\n",
    "\n",
    "\n",
    "        #else:\n",
    "        #    if \"kernel_initializer\" in model3.layers[n].get_config():\n",
    "        #        initializer = model3.layers[n].kernel_initializer\n",
    "        #        reinitialize_layer(model3, initializer, model3.layers[n].name, trainable=True)\n",
    "    #    else:\n",
    "    #        reinitialize_layer(model3, initializer0, model3.layers[n].name, trainable=True)\n",
    "                                \n",
    "\n",
    "    ######################\n",
    "    # print(model1.layers[-1].get_config())\n",
    "    # print(model2.layers[-1].get_config())\n",
    "    # print(model3.layers[-1].get_config())\n",
    "        #print(model0.optimizer.get_config())\n",
    "        #print(model1.optimizer.get_config())\n",
    "        #print(model2.optimizer.get_config())\n",
    "    # print(model1.loss())\n",
    "    # print(model2.loss.get_config())\n",
    "    # print(model3.loss.get_config())\n",
    "    #########\n",
    "        input_shape = (window_size,xtrain.shape[2],1)\n",
    "    # print(input_shape)\n",
    "        input_ = Input(shape=input_shape, name=\"img\")\n",
    "        model0._name = model0.name + \"_0\"\n",
    "        model1._name = model1.name + \"_1\"\n",
    "        model2._name = model2.name + \"_2\"\n",
    "    # \n",
    "    # inpSeparate1 = Lambda(lambda x: x[0])(input_)\n",
    "        model0_ = model0#(input_)\n",
    "    # inpSeparate2 = Lambda(lambda x: x[1])(input_)\n",
    "        model1_ = model1#(input_)\n",
    "    # inpSeparate3 = Lambda(lambda x: x[2])(input_)\n",
    "        model2_ = model2#(input_)\n",
    "        for n, layer in enumerate(model0.layers):\n",
    "            layer._name = layer.name + str(\"_0\")\n",
    "        \n",
    "        for n, layer in enumerate(model1.layers):\n",
    "            layer._name = layer.name + str(\"_1\")\n",
    "        \n",
    "    for n, layer in enumerate(model2.layers):\n",
    "        layer._name = layer.name + str(\"_2\")\n",
    "    conc = Concatenate()([model0.layers[-1].output, model1.layers[-1].output, model2.layers[-1].output])\n",
    "    dense = Dense(3, name=\"dense012\")(conc)\n",
    "    output = Dense(3, activation='softmax', name=\"output012\")(dense)\n",
    "    #output2 = Dense(1, activation='sigmoid', name=\"1\")(model2.layers[-1].output)\n",
    "    #output3 = Dense(1, activation='sigmoid', name=\"2\")(model3.layers[-1].output)\n",
    "    model = Model([model0.input, model1.input, model2.input], output)\n",
    "    #model = Model([model1.input, model2.input, model3.input], [output1, output2, output3])\n",
    "\n",
    "    #model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_crossentropy\"])\n",
    "    #config = model.to_json()\n",
    "    #loaded_model = tensorflow.keras.models.model_from_json(config)\n",
    "\n",
    "    #losses = {xn: \"categorical_crossentropy\" for n, xn in enumerate(self.outputs)}\n",
    "\n",
    "    #metrics = {xn: \"categorical_crossentropy\" for n, xn in enumerate(self.outputs)}\n",
    "\n",
    "    #lossWeights = {xn: 1.0 for n, xn in enumerate(self.outputs)}\n",
    "    INIT_LR =  1e-04 #/ 10\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=INIT_LR)#, decay=INIT_LR / self.epochs)#, epsilon=1e-05)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]) #loss=losses, loss_weights=lossWeights, optimizer=tf.keras.optimizers.Adam(),#learning_rate=INIT_LR),#, decay=INIT_LR / self.epochs, epsilon=1e-05),\n",
    "              # metrics=metrics) #epsilon=1e-07\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3467df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    weight_for_0 = (1 / np.sum(ytrain[:,0]))*(ytrain.shape[0])/3.0\n",
    "    weight_for_1 = (1 / np.sum(ytrain[:,1]))*(ytrain.shape[0])/3.0\n",
    "    weight_for_2 = (1 / np.sum(ytrain[:,2]))*(ytrain.shape[0])/3.0\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "    print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b3728",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "    ytrainN = np.zeros(ytrain.shape)\n",
    "    ytrainN[:,0] = ytrain[:,1]\n",
    "    ytrainN[:,1] = ytrain[:,0]\n",
    "    ytrainN[:,2] = ytrain[:,2]\n",
    "\n",
    "    yvalN = np.zeros(yval.shape)\n",
    "    yvalN[:,0] = yval[:,1]\n",
    "    yvalN[:,1] = yval[:,0]\n",
    "    yvalN[:,2] = yval[:,2]\n",
    "\n",
    "    ytestN = np.zeros(ytest.shape)\n",
    "    ytestN[:,0] = ytest[:,1]\n",
    "    ytestN[:,1] = ytest[:,0]\n",
    "    ytestN[:,2] = ytest[:,2]\n",
    "\n",
    "    filepathBestModel = os.path.join(PathSaveFinals, \"model_autokerasBestModel\")\n",
    "    filenamecsv1 = os.path.join(PathSaveFinals, \"CSVLOGhistoryTrain.csv\")\n",
    "    callback = [tf.keras.callbacks.CSVLogger(filenamecsv1, separator=',', append=False),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                                    filepathBestModel,\n",
    "                                    monitor='val_accuracy',\n",
    "                                    mode='max')\n",
    "            ]\n",
    "    sh0T,sh1T,sh2T,sh3T = xtrain.shape[0], xtrain.shape[1], xtrain.shape[2], xtrain.shape[3]\n",
    "    sh0V,sh1V,sh2V,sh3V = xval.shape[0], xval.shape[1], xval.shape[2], xval.shape[3]\n",
    "    history = model.fit([xtrain[:,:,:,1].reshape((sh0T,sh1T,sh2T,1)),xtrain[:,:,:,0].reshape((sh0T,sh1T,sh2T,1)),xtrain[:,:,:,2].reshape((sh0T,sh1T,sh2T,1))], ytrain, validation_data=([xval[:,:,:,1].reshape((sh0V,sh1V,sh2V,1)), xval[:,:,:,0].reshape((sh0V,sh1V,sh2V,1)),xval[:,:,:,2].reshape((sh0V,sh1V,sh2V,1))], yval), epochs=epochs, class_weight=class_weight, callbacks=callback)\n",
    "    #model = model.export_model()\n",
    "    print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>\n",
    "\n",
    "    \n",
    "    try:\n",
    "        model.save(os.path.join(PathSaveFinals,\"model_autokerasWS\" + str(window_size)), save_format=\"tf\")\n",
    "    except:\n",
    "        model.save(os.path.join(PathSaveFinals,\"model_autokerasWS\" + str(window_size) + \".h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b61465",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = auto_model.export_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4253ae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93059b70",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    for layer in model.layers:\n",
    "        print(layer.get_config())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(os.path.join(PathSaveFinals, \"PlotsTrain\")):\n",
    "        os.mkdir(os.path.join(PathSaveFinals, \"PlotsTrain\"))\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.savefig(os.path.join(PathSaveFinals, \"PlotsTrain\", \"PlotLossHistory.png\"))\n",
    "    print(\"Saved plot loss in: \" + os.path.join(PathSaveFinals, \"PlotsTrain\", \"PlotLossHistory.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0f5ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "else:\n",
    "    sh0tt,sh1tt,sh2tt,sh3tt = xtest.shape[0], xtest.shape[1], xtest.shape[2], xtest.shape[3]\n",
    "    model = load_model(os.path.join(DataModelTrain,\"model_autokerasBestModel\"))\n",
    "    if len(G) > 1:\n",
    "        predicted_y = model.predict([xtest[:,:,:,1].reshape((sh0tt,sh1tt,sh2tt,1)),xtest[:,:,:,0].reshape((sh0tt,sh1tt,sh2tt,1)),xtest[:,:,:,2].reshape((sh0tt,sh1tt,sh2tt,1))], use_multiprocessing=True, batch_size=batch_size)\n",
    "    else:\n",
    "        predicted_y = model.predict([xtest[:,:,:,1].reshape((sh0tt,sh1tt,sh2tt,1)),xtest[:,:,:,0].reshape((sh0tt,sh1tt,sh2tt,1)),xtest[:,:,:,2].reshape((sh0tt,sh1tt,sh2tt,1))])\n",
    "\n",
    "    roc_data = dict()\n",
    "    auc = dict()\n",
    "    optth = dict()\n",
    "    for i in range(nb_classes):\n",
    "        roc_data[i] = metrics.roc_curve(ytest[:, i], predicted_y[:, i])\n",
    "        auc[i] = metrics.auc(roc_data[i][0], roc_data[i][1])\n",
    "        fpr, tpr, thr = metrics.roc_curve(ytest[:, i], predicted_y[:, i])\n",
    "        i_opt = np.argmin([np.linalg.norm((1 - ti, fi)) for (ti, fi) in zip(tpr, fpr)])\n",
    "        optth[i] = thr[i_opt]\n",
    "        print(auc[i])\n",
    "\n",
    "    f, ax = plt.subplots(figsize=[9, 6])\n",
    "    ax.plot(roc_data[0][0], roc_data[0][1], 'k-', label='class 0, AUC = {:4.2f}'.format(auc[0]))\n",
    "    ax.plot(roc_data[1][0], roc_data[1][1], 'b-', label='class 1, AUC = {:4.2f}'.format(auc[1]))\n",
    "    ax.plot(roc_data[2][0], roc_data[2][1], 'r-', label='class 2, AUC = {:4.2f}'.format(auc[2]))\n",
    "    # ax.plot([0, 1], [0, 1], 'g--')\n",
    "    ax.legend(loc='lower right')\n",
    "    f.savefig(os.path.join(PathSaveFinals, \"RocClasses.png\"))\n",
    "\n",
    "    APS_data = dict()\n",
    "    APS = dict()\n",
    "    for i in range(nb_classes):\n",
    "        # APS_data[i] = metrics.roc_curve(y_test, preds)\n",
    "        APS_data[i] = metrics.precision_recall_curve(ytest[:, i], predicted_y[:, i])\n",
    "        APS[i] = metrics.average_precision_score(ytest[:, i], predicted_y[:, i])\n",
    "\n",
    "    f, ax = plt.subplots(figsize=[9, 6])\n",
    "    ax.plot(APS_data[0][0], APS_data[0][1], 'k-', label='class 0, AUC = {:4.2f}'.format(APS[0]))\n",
    "    ax.plot(APS_data[1][0], APS_data[1][1], 'b-', label='class 1, AUC = {:4.2f}'.format(APS[1]))\n",
    "    ax.plot(APS_data[2][0], APS_data[2][1], 'r-', label='class 2, AUC = {:4.2f}'.format(APS[2]))\n",
    "    ax.legend(loc='lower right')\n",
    "    f.savefig(os.path.join(PathSaveFinals, \"Precision_recall.png\"))\n",
    "    \n",
    "    TRUEY = np.float32(ytest)\n",
    "    TRUEY = np.argmax(TRUEY, axis=1)#[np.argmax(p, axis=1) for i,p in enumerate(TRUEY)]\n",
    "    #PREDY = np.float32(predicted_y)\n",
    "    #PREDY = np.argmax(PREDY, axis=1) #[np.argmax(p, axis=1) for i, p in enumerate(PREDY)]\n",
    "    #PREDY = np.zeros((predicted_y.shape[0],3))\n",
    "    #for i in range(3):\n",
    "    #    for n,cl in enumerate(predicted_y[:,i]):\n",
    "    #        if cl < optth[i]:\n",
    "    #            PREDY[n,i] = 1.0\n",
    "    PREDY = np.float32(predicted_y)\n",
    "    PREDY = np.argmax(PREDY, axis=1)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(TRUEY,PREDY, normalize=\"true\"))#, labels=labels)\n",
    "    np.savetxt(os.path.join(PathSaveFinals, \"CM.npy\"), confusion_matrix(TRUEY,PREDY,normalize=\"true\"), delimiter=\",\")\n",
    "    kkkkk\n",
    "    CMatrix = {'Test':dict()}\n",
    "    CMatrix['Test'] = cm_analysis(y_true=TRUEY,\n",
    "                                         y_pred=PREDY,\n",
    "                                         filename=os.path.join(PathSaveFinals, \"PlotsTrain\",'ConfMatrix_Test_'),\n",
    "                                         classes=Ylevels,\n",
    "                                         labels=np.arange(0,3,1),\n",
    "                                         figsize=(20,20),\n",
    "                                         cmap='Purples') \n",
    "    print(\"Saved plot CM in: \" + os.path.join(PathSaveFinals, \"PlotsTrain\",'ConfMatrix_Test_.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc88e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcfc59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
